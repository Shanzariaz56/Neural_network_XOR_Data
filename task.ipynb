{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first initialized required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X  Y  Z\n",
      "0     0  0  0\n",
      "1     0  1  1\n",
      "2     1  1  0\n",
      "3     1  1  0\n",
      "4     0  0  0\n",
      "...  .. .. ..\n",
      "9995  0  0  0\n",
      "9996  0  1  1\n",
      "9997  1  1  0\n",
      "9998  1  1  0\n",
      "9999  1  1  0\n",
      "\n",
      "[10000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#load XOR dataset\n",
    "data=pd.read_csv(\"Xor_Dataset.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extration \n",
    "inputs=data[[\"X\",\"Y\"]].to_numpy()\n",
    "output_expected=data[[\"Z\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define the activation function \n",
    "# here am using sigmoid as a activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can take derivative of sigmoid function for backpropagation\n",
    "def derivative_sigmoid(x):\n",
    "    return x*(1-x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Initialize layer sizes\n",
    "input_layer_neuron = 2\n",
    "hidden_layer1_neuron = 3\n",
    "hidden_layer2_neuron = 3\n",
    "output_layer_neuron = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier Initialization for weights\n",
    "#hidden_layer_1\n",
    "weight_input_hidden1=np.random.randn(input_layer_neuron,hidden_layer1_neuron)*np.sqrt(1./input_layer_neuron)\n",
    "bias_hidden1=np.zeros((1,hidden_layer1_neuron))\n",
    "#hidden_layer_2\n",
    "weight_hidden1_hidden2=np.random.randn(hidden_layer1_neuron,hidden_layer2_neuron)*np.sqrt(1./hidden_layer1_neuron)\n",
    "bias_hidden2=np.zeros((1,hidden_layer2_neuron))\n",
    "#output_layer\n",
    "weight_hidden2_output=np.random.randn(hidden_layer2_neuron,output_layer_neuron)*np.sqrt(1./hidden_layer2_neuron)\n",
    "bias_output=np.zeros((1,output_layer_neuron))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now apply forward_propagation\n",
    "def forward_propagation(inputs):\n",
    "    #hidden_layer_1\n",
    "    hidden1_input=np.dot(inputs,weight_input_hidden1)+bias_hidden1\n",
    "    hidden1_output=sigmoid(hidden1_input)\n",
    "    #hidden_layer_2\n",
    "    hidden2_input=np.dot(hidden1_output,weight_hidden1_hidden2)+bias_hidden2\n",
    "    hidden2_output=sigmoid(hidden2_input)\n",
    "    #Output_layer\n",
    "    output_input=np.dot(hidden2_output,weight_hidden2_output)+bias_output\n",
    "    predicted_output=sigmoid(output_input)\n",
    "    \n",
    "    return predicted_output, hidden1_output, hidden2_output\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(inputs,output_expected,predicted_output,hidden1_output,hidden2_output,learning_rate):\n",
    "    global weight_input_hidden1,bias_hidden1\n",
    "    global weight_hidden1_hidden2,bias_hidden2\n",
    "    global weight_hidden2_output,bias_output\n",
    "    \n",
    "    error=predicted_output-output_expected\n",
    "    #now calculate the gradient descent od output and hiddern layers\n",
    "    output_gradient=error*derivative_sigmoid(predicted_output)\n",
    "    hidden2_gradient=np.dot(output_gradient,weight_hidden2_output.T)*derivative_sigmoid(hidden2_output)\n",
    "    hidden1_gradient=np.dot(hidden2_gradient,weight_hidden1_hidden2.T)*derivative_sigmoid(hidden1_output)\n",
    "    \n",
    "    #now update wight and bias\n",
    "    #output layer weight and bias is updated\n",
    "    weight_hidden2_output-=learning_rate*np.dot(hidden2_output.T, output_gradient)\n",
    "    bias_output-=learning_rate*np.sum(output_gradient, axis=0, keepdims=True)\n",
    "    \n",
    "    #update weight of hidden layer 2\n",
    "    weight_hidden1_hidden2-=learning_rate*np.dot(hidden1_output.T,hidden2_gradient)\n",
    "    bias_hidden2-=learning_rate*np.sum(hidden2_gradient, axis=0, keepdims=True)\n",
    "    \n",
    "    #update weight of hidden layer 3\n",
    "    weight_input_hidden1-=learning_rate*np.dot(inputs.T,hidden1_gradient)\n",
    "    bias_hidden1-=learning_rate*np.sum(hidden1_gradient, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter and training loop\n",
    "learning_rate=0.001\n",
    "epochs=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make accuracy function \n",
    "def accuracy(y_true,y_predicted):\n",
    "    #first convert all the prediction into the binary as \"0\",\"1\"\n",
    "    prediction_binary=(y_predicted>=0.5).astype(int)\n",
    "    #now calculate total prediction\n",
    "    total_prediction=len(prediction_binary)\n",
    "    #now calculate the true prediction\n",
    "    true_prediction=np.sum(y_true==prediction_binary)\n",
    "    #here is the formula to calculate accuracy\n",
    "    accuracy_formula=(true_prediction/total_prediction)*100\n",
    "    return accuracy_formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7156340318258991, Accuracy: 49.519999999999996\n",
      "Epoch 500, Loss: 0.6929453430603524, Accuracy: 50.480000000000004\n",
      "Epoch 1000, Loss: 0.08701866794004963, Accuracy: 100.0\n",
      "Epoch 1500, Loss: 0.017172510218178824, Accuracy: 100.0\n",
      "Epoch 2000, Loss: 0.011423864511885254, Accuracy: 100.0\n",
      "Epoch 2500, Loss: 0.009040303526769856, Accuracy: 100.0\n",
      "Epoch 3000, Loss: 0.007676486703969587, Accuracy: 100.0\n",
      "Epoch 3500, Loss: 0.0067716605655636156, Accuracy: 100.0\n",
      "Epoch 4000, Loss: 0.006117558180636498, Accuracy: 100.0\n",
      "Epoch 4500, Loss: 0.005617394285980756, Accuracy: 100.0\n",
      "Epoch 5000, Loss: 0.005219481382729915, Accuracy: 100.0\n",
      "Epoch 5500, Loss: 0.00489344982222933, Accuracy: 100.0\n",
      "Epoch 6000, Loss: 0.004620163148322557, Accuracy: 100.0\n",
      "Epoch 6500, Loss: 0.004386900264494588, Accuracy: 100.0\n",
      "Epoch 7000, Loss: 0.004184839135148478, Accuracy: 100.0\n",
      "Epoch 7500, Loss: 0.0040076479970686425, Accuracy: 100.0\n",
      "Epoch 8000, Loss: 0.0038506513777512553, Accuracy: 100.0\n",
      "Epoch 8500, Loss: 0.003710313232241483, Accuracy: 100.0\n",
      "Epoch 9000, Loss: 0.0035839040844372577, Accuracy: 100.0\n",
      "Epoch 9500, Loss: 0.003469279560536171, Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "#now train thw data\n",
    "for i in range(epochs):\n",
    "    #for forward_propagation\n",
    "    predicted_output,hidden1_output,hidden2_output=forward_propagation(inputs)\n",
    "    #for loss\n",
    "    loss=binary_cross_entropy_loss(output_expected,predicted_output)\n",
    "    #now for backward_propagation\n",
    "    backward_propagation(inputs,output_expected,predicted_output,hidden1_output,hidden2_output,learning_rate)\n",
    "    if i % 500 == 0:\n",
    "        calculate_accuracy=accuracy(output_expected,predicted_output)\n",
    "        print(f\"Epoch {i}, Loss: {loss}, Accuracy: {calculate_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Outputs:\n",
      "[[0.00370917]\n",
      " [0.99646594]\n",
      " [0.00315684]\n",
      " [0.00315684]\n",
      " [0.00370917]\n",
      " [0.99694713]\n",
      " [0.00315684]\n",
      " [0.00315684]\n",
      " [0.99646594]\n",
      " [0.99694713]]\n"
     ]
    }
   ],
   "source": [
    "#test result\n",
    "test_inputs = inputs[:10]\n",
    "predicted_test_output, _, _ = forward_propagation(test_inputs)\n",
    "print(f\"Test Outputs:\\n{predicted_test_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model.save\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define your model parameters (weights and biases)\n",
    "model_data = {\n",
    "    \"weight_input_hidden1\": weight_input_hidden1,\n",
    "    \"bias_hidden_1\": bias_hidden1,\n",
    "    \"weight_hidden1_hidden2\": weight_hidden1_hidden2,\n",
    "    \"bias_hidden_2\": bias_hidden2,\n",
    "    \"weight_hidden2_output\": weight_hidden2_output,\n",
    "    \"bias_output\": bias_output\n",
    "}\n",
    "\n",
    "# Save model data to a .save file\n",
    "with open('model.save', 'wb') as f: \n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"Model saved as model.save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model data: {'weight_input_hidden1': array([[-5.9273479 , -3.49670878,  3.15435433],\n",
      "       [ 5.9203967 ,  3.38525153, -2.96250047]]), 'bias_hidden_1': array([[ 3.12183989, -1.84085354,  1.70674388]]), 'weight_hidden1_hidden2': array([[ 4.39904925,  3.21570264, -3.92046361],\n",
      "       [-3.23775944, -1.4374024 ,  4.04166778],\n",
      "       [ 1.07471539, -0.43891009, -2.7943528 ]]), 'bias_hidden_2': array([[-2.99578637, -0.8224644 ,  3.56812116]]), 'weight_hidden2_output': array([[-6.87261607],\n",
      "       [-3.52127546],\n",
      "       [ 8.72085601]]), 'bias_output': array([[2.13460498]])}\n"
     ]
    }
   ],
   "source": [
    "# Load the model data from the .save file\n",
    "with open('model.save', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Now you can access the model parameters from loaded_model\n",
    "print(\"Loaded model data:\", loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
